# -*- coding: utf-8 -*-
"""step2d_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCLNvu1atyAhcd-CEjBFt7iiXMlN1jDy
"""

!pip install transformers

from tqdm import tqdm
from transformers import logging
from transformers import pipeline
from transformers import BertTokenizer, TFBertModel
import glob
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import pickle
import numpy as np
import random

logging.set_verbosity(logging.ERROR)

base_dir = "/content/drive/MyDrive/Courage_GCN_HS/dataset/"


tokenizer = None
pipe = None



# tokenizer = BertTokenizer.from_pretrained(bert_model_name)
# pipe = pipeline('feature-extraction', model=bert_model_name, tokenizer=tokenizer)

# preds = pipe([str_words])[0]



def __init(lang):
  bert_model_name = None
  if lang =="en":
    bert_model_name = "bert-base-uncased"
  elif lang =="es":
    bert_model_name = "dccuchile/bert-base-spanish-wwm-uncased"  # BETO
  if lang =="de":
    bert_model_name = "bert-base-german-dbmdz-uncased"
  if lang =="it":
    bert_model_name = "dbmdz/bert-base-italian-uncased"
  print("initializing", bert_model_name, "for", lang)
  tokenizer = BertTokenizer.from_pretrained(bert_model_name)
  pipe = pipeline('feature-extraction', model=bert_model_name, tokenizer=tokenizer)
  return pipe, tokenizer

def get_embeddings(ds, lang):
  embeddings = {}
  pipe, tokenizer = __init(lang)
  for sent in tqdm(ds):
    if sent["lang"] != lang:
      continue
    words = [word["text"] for word in sent["parsed"]]
    if len(words)>300:
      tks = tokenizer.tokenize(" ".join(words))
      if len(tks)>510:
        tks = tks[:510] # CLS and SEP
        tks = " ".join(tks)
        tks = tks.replace(" ##","")
        words = tks.split() 
    words = " ".join(words)
    preds = pipe([words])[0]
    # print(len(preds), len(preds[0]))
    embeddings[sent["id"]] = np.asarray(preds)
  return embeddings



"""#HASOC 2021

"""

print(glob.glob(base_dir + "en_hasoc2021_train.csv.pkl"))
for pkl in glob.glob(base_dir + "en_hasoc2021_train.csv.pkl"):
  print(pkl)
  if len(pkl.split("pkl"))>2:
    print("...skiping", pkl)
    continue
  # if pkl in skip:
  #   print("...skiping (b)", pkl)
  #   continue
  ds, _ = pickle.load(open(pkl, 'rb'))
  langs = set([sent["lang"] for sent in ds])
  for lang in langs:
    new_ds = get_embeddings(ds, lang)
    val = -1
    for v in ds:
      if v["lang"] == lang:
        val = v["id"]
        break
    print("...", new_ds[val].shape)
    pickle.dump(new_ds, open(pkl.replace(".pkl","") +"_bert_"+ lang +".pkl", 'wb'))

def plot_file(bert, cls):
  X = []
  y = []
  used = set()
  for id in bert:
    if random.uniform(0, 1) < .5 or id in used:
      continue
    used.add(id)
    # print(bert[id][0])
    v = bert[id]
    v = np.mean(v, axis=0)
    X.append(v)
    c = -1
    if cls[id] == "NONE":
      c = 0
    elif cls[id] == "OFFN":
      c = 1
    elif cls[id] == "PRFN":
      c = 2
    y.append(c)
    if len(y) > len(bert)/4:
      break
  print("showing",len(X), "total",len(bert))
  X_embedded = TSNE(n_components=2, verbose = 1).fit_transform(X)
  y = np.asarray(y)
  plt.figure(figsize=(6, 5))
  # plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c="g", label="all")
  plt.scatter(X_embedded[y == 0, 0], X_embedded[y == 0, 1], c="b", label="not")
  plt.scatter(X_embedded[y == 1, 0], X_embedded[y == 1, 1], c="r", label="hate")
  plt.scatter(X_embedded[y == 2, 0], X_embedded[y == 2, 1], c="g", label="hate")
  
  plt.legend()
  plt.show()

print(glob.glob(base_dir + "en_hasoc2021_train.csv.pkl"))
files = glob.glob(base_dir + "en_hasoc2021_train.csv.pkl")
for file_path in files:
  ds, _ = pickle.load(open(file_path, 'rb'))
  cls = {}
  for item in ds:
    cls[item["id"]] = item["cls"]
  print("cls len", len(cls))
  file_path = file_path.replace(".pkl", "") + "*_bert_*.pkl"
  lst_file_path = glob.glob(file_path)
  for path in lst_file_path:
    print(path)
    bert = pickle.load(open(path, 'rb'))
    plot_file(bert, cls)