# -*- coding: utf-8 -*-
"""step2a_clean+parser HASOC vTestset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDJRhgkQwgfJ0dEjlus7szrHKkvWB5To
"""

# !pip install tweet-preprocessor
# !pip install stanza
# !pip install emoji

import stanza
from tqdm import tqdm

import emoji as emoji_api 
import preprocessor as tweet_preprocessor
import pickle
import csv

stanza.download('en')

nlp_en = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')

ds_files = ["/content/drive/MyDrive/Courage_GCN_HS/dataset/original/hasoc2021/en_Hasoc2021_test_task1.csv"]
process_testset = "test"
tweet_preprocessor.set_options(tweet_preprocessor.OPT.EMOJI,#Emoji
                               tweet_preprocessor.OPT.SMILEY,#Smiley
                               )

!wget "https://raw.githubusercontent.com/valeriobasile/hurtlex/master/lexica/EN/1.2/hurtlex_EN.tsv"

def __process_hurtlex(file_path, conservative_only=True):
  hurtlex = {}  
  with open(file_path) as input_file:
    for ln in input_file:
      id, pos, category, stereotype, lemma, level = ln.strip().split("\t")
      if level == 'conservative' or not conservative_only:
        hurtlex[pos+"_"+lemma] = category
  return hurtlex

hurtlex_en = __process_hurtlex("hurtlex_EN.tsv")

LST_SYMBOLS = ['â¤µ', 'à¥€', 'à¯', 'à§', 'à¤¿', 'â€¡', 'à¥‚', 'à¥ˆ', 'Ìˆ', 'Ø‘', 'â—', 'à¥‹', 'à¥', 'à²¾', 'Â¦', 'â–ª', 'à³', 'Â°', 'à¼„', 'à³', 'â€ ', 'Â¤', 'à®¾', 'à¥‰', 'à¦¿', 'â†º', 'à¦‚', 'à¸·', 'â€°', 'â”Œ', 'â†“', 'â”', 'Â¯', 
               'Â¸', 'à¦¾', 'â—†', 'Â¬', 'Ë½', 'â€¿', 'à¯', 'à¸µ', 'à¤‚', 'à¹ˆ', 'à¥‡', 'âˆ', 'Â¨', 'à³†', 'Â±', 'â€“', 'â¬‡', '\\', 
               'à¤¾', 'à²‚', 'â—‹', 'à¥¤', ')', 'à¥', 'â‹†', 'à®¿ ','â†’', 'à§‡', 'à¤¼', 'â–¡', 'à²¿']
MAP_LETTERS = {'ğŸ‡´':"O", 'ğŸ‡º':"U", 'ğŸ†—':"ok", 'ğŸ‡¬':"G", 'ğŸ‡¨':"C", 'ğŸ‡«':"F", 'ğŸ‡¾':"Y", 'ğŸ‡²':"M", 'ğŸ‡¹':"T", 'ğŸ‡®':"I", 'ğŸ…·':"H", 'ğŸ‡°':"K", 'ğŸ‡­':"H", 'â¬':"V", 'ğŸ‡¯':"J", 'ğŸ†š':"VS", 'ğŸ‡ª':"E", 'ğŸ‡©':"D", 'ğŸ‡¦':"A", 'ğŸ‡¸':"S", 'ğŸ‡¶':"Q", 'ğŸ†ƒ':"T", 'ğŸ…°':"A", 'ğŸ…¸':"I", 'ğŸ‡·':"R", 'ğŸ‡»':"V", 'ğŸ††':"W", 'ğŸ‡µ':"P", 'âƒ£':" ", 'ğŸ†‚':"W", 'ğŸ‡§':"B", 'ğŸ‡½':"X", 'ğŸ†˜':"SOS", 'ğŸ‡³':"n", 'ğŸ…‚':"S", 'ğŸ‡±': "L"}
MAP_OTHER = {'Í¡':":(", 'â€¹':"<", 'ã€‹':">", 'ã€':"\"", '\x81':" ", '\u202a':" ", '\U000e0065':" ", '\u200b':" ", 'ã€Š':"<", 'Â·':"-", 'â”€':"-", 'Ã—':"x", 
             '\U000e0074':" ", 'ğŸ„¸':"I", 'âˆ™':"-", 'â€˜':"'", 'â€¢':"-", 'â€œ':"\"", 'Â»':">", 'ã€':"'", 'â€™':"'", 'Ø›':";", 'â€¼':"!", 'Ì¶':"-", 'â‰':"?", 'â„…':"%",
             '~':"~", 'â€”':"-", 'â€':"\"", 'Â«':"<", 'Â¶':"paragraph", 'Â§':"paragraph", 'â€š':",", 'â‰ ':"different",'â„¢':"Trademark", 'â€¦':".", '\u2066':" ", 
             '\u2060':" ", '\u200d':" ", '\U000e0063':" ", '\U000e0067':" ", '\u2063':" ", '\U000e0073':" ", '\U000e006e':" ", '\U000e0062':" ", '\U000e007f':" ", 
             '\xad':" ", '\u2069':" ", 'ã€':"[", 'â—„':"<", 'Â®':"registered", 'ãƒ»':"-", 'Â´':"'", 'Â©':"copyright", 'â–º':">", 'Íœ':":)", 'â”':"-", 'Ëœ':"~", 'ï¼':"!", 'â €':" ", 
             'â€•':"-", '`':"'", 'ï¼Œ':",", 'Ù':"-", 'â–¶':">", 'â€':"\"", 'ã€‘':"]", 'ã€':"\"", 'ï¼Ÿ':"?",   }
def __clean(ln):
  if ln[0] == "\"" and ln[-1] == "\"":
    ln = ln[1:-1]
  for symbol in LST_SYMBOLS:
    ln = ln.replace(symbol, " ")
  for symbol in MAP_LETTERS:
    ln = ln.replace(symbol, " ")
  for symbol in MAP_OTHER:
    ln = ln.replace(symbol, " ")
  ln = ln.replace("â€™","'").replace("â€˜","'").replace("Â´","'").replace("â€œ",'"').replace("â€",'"').replace("â€¦",".").replace("ã…¤","-").replace("â‹†","*")
  ln = ln.replace("â€“","-").replace("â€”","-").replace("â©","-").replace("â",'"').replace("â¤","*").replace("ï¸","*").replace("â—","*").replace("ã€","[").replace("ã€‘","]")
  ln = ln.replace("??â€?","?").replace("??â€","?").replace("!!!","!").replace("!!","!").replace("â",'"').replace("âš˜","*").replace("âœ“","v").replace("âœ–","x")
  ln = ln.replace("â¬‡","!").replace("â¦","")
  ln = ln.replace("\u200b","").replace("\u202a","")
  return ln

def __make_real(text):
  sent = []
  lst_replaces = []
  for word in text.split():
    if word[0] == "$" and word[-1] == "$" and len(word)>5:
      word = word[1:-1]
    if word.startswith("PERSON"):
      lst_replaces.append(word)
      word = "PERSON"
    sent.append(word)
  return " ".join(sent), lst_replaces

LST_EMOJIS = ['ğŸ§', 'ğŸ¤«', 'ğŸ¤®', 'ğŸ¤”', 'â±', 'ğŸ¥‡', 'ğŸ¤˜', 'ğŸ¤©', 'â­', 'ğŸ§ª', 'ğŸ§¢', 'ğŸ§ ', 'ğŸ¤§', 'ğŸ¥¾', 'ğŸ§•', 'ğŸ¤¥', 'ğŸ¦†', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ¤', 'ğŸ§', 'â°', 'ğŸ¥‹', 'ğŸ¥º', 'ğŸ¤‘', 'ğŸ¤¦', 'â³', 'ğŸ¤¤', 'ğŸ¥¼', 'ğŸ¥”', 'ğŸ¥°', 'ğŸ¤ ', 'ğŸ¤¨', 'ğŸ¥³', 'ğŸ¤“', 'ğŸ¦', 'ğŸ¤—', 'ğŸ¦…', 'ğŸ§š', 'ğŸ¥š', 'ğŸ¤', 'ğŸ¥‚', 'ğŸ¤', 'ğŸ¤²', 'ğŸ¤¢', 'ğŸ§”', 'ğŸ¤£', 'ğŸ¤Ÿ', 'ğŸ§™', 'ğŸ¤¡', 'ğŸ§€', 'ğŸ¥™', 'ğŸ§›', 'ğŸ¤›', 'ğŸ¤¸', 'ğŸ¥', 'ğŸ¤¯', 'ğŸ§¤', 'ğŸ¤™', 'ğŸ¥Š', 'ğŸ¤­', 'ğŸ¤ª', 'ğŸ¤·', 'ğŸ¤¬']

def load_ds(ds_file):
  total_sents = len(open(ds_file).readlines())
  my_ds = []
  err = []
  dsname = ds_file.replace("only_","").replace(".csv","")
  with open(ds_file, newline='') as input_file_f:
    csv_reader = csv.reader(input_file_f, delimiter=',')
    for index, ln in tqdm(enumerate(csv_reader), total=total_sents):
      lang = "en"
      ds = "hasoc2021"
      splitset = "train"
      source = "twitter"
      if process_set == "test":
        original_id, txt = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = "NONE"
      elif process_set == "train1":
        original_id, txt,task_1, task_2 = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = task_1
      elif process_set == "train2":
        original_id, txt,task_1, task_2 = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = task_2
      id = index
      all_words = []
      # this_sent = {"lang":lang, "ds":ds, "id":dsname+"_"+lang+"_"+str(id), "original_id":original_id,"cls":cls, "splitset":splitset, "source":source, "orignal_sent":txt, "parsed":all_words}
      this_sent = {"lang":lang, "ds":ds, "id":original_id, "original_id":original_id,"cls":cls, "splitset":splitset, "source":source, "orignal_sent":txt, "parsed":all_words}
      my_ds.append(this_sent)
      txt = __clean(txt)
      txt, replaces = __make_real(txt)
      # print(replaces)
      # try:
      if True:
        parser = nlp_en
        hurtlex = hurtlex_en
        txt = txt.replace("$HASHTAG$","HASHTAG")
        parsed = parser(txt)
        for sent in parsed.sentences:
          for word in sent.words: # for word, token in zip(sent.words, sent.tokens):
            # print(token)
            text = word.text
            lemma = word.lemma
            pos = word.upos
            feats = (word.feats if word.feats else "_")
            # ner = token.ner
            dep = word.deprel
            head = word.head
            
            if text == "PERSON":
              text = replaces.pop(0)
            if lemma and (pos == "NOUN" or pos == "ADJ"):
              if pos[0].lower() + "_" + lemma in hurtlex:
                hurt = hurtlex[pos[0].lower() + "_" + lemma]
              else:
                hurt = "NA"
            else:
              hurt = "NA"
            emoji_api_lg = lang
            if emoji_api_lg == "de":
              emoji_api_lg = "en"
            if text in LST_EMOJIS:
              text = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
              lemma = "EMOJI"
            else:
              aux = tweet_preprocessor.parse(text)
              if aux.emojis:
                for val in aux.emojis:
                  emoji = val.match
                  newemoji = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
                  text = emoji
                  lemma = "EMOJI"
              elif aux.smileys:
                for val in aux.smileys:
                  emoji = val.match
                  newemoji = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
                  text = emoji
                  lemma = "EMOJI"
            if text in MAP_LETTERS:
              text = MAP_LETTERS[text]
              lemma = "LETTER"
            this_word = {"text":text, "lemma":lemma, "pos":pos, "feats":feats, "dep":dep,"head":head, "hurtlex": hurt}
            all_words.append(this_word)
        # print(replaces, all_words)
        assert len(replaces) == 0
      # except Exception as inst:
      #   print(inst.args)
      #   print(inst)  
      #   err.append(ln)
  return my_ds, err

for file_path in ds_files:
  print(file_path)
  # print(file_path+".pkl")
  ds, err = load_ds(file_path)
  print("...errs", len(err))
  pickle.dump([ds, err], open(file_path+".pkl", 'wb'))



