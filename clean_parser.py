# -*- coding: utf-8 -*-
"""step2a_clean+parser HASOC vTestset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDJRhgkQwgfJ0dEjlus7szrHKkvWB5To
"""

# !pip install tweet-preprocessor
# !pip install stanza
# !pip install emoji

import stanza
from tqdm import tqdm

import emoji as emoji_api 
import preprocessor as tweet_preprocessor
import pickle
import csv

stanza.download('en')

nlp_en = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')

ds_files = ["/content/drive/MyDrive/Courage_GCN_HS/dataset/original/hasoc2021/en_Hasoc2021_test_task1.csv"]
process_testset = "test"
tweet_preprocessor.set_options(tweet_preprocessor.OPT.EMOJI,#Emoji
                               tweet_preprocessor.OPT.SMILEY,#Smiley
                               )

!wget "https://raw.githubusercontent.com/valeriobasile/hurtlex/master/lexica/EN/1.2/hurtlex_EN.tsv"

def __process_hurtlex(file_path, conservative_only=True):
  hurtlex = {}  
  with open(file_path) as input_file:
    for ln in input_file:
      id, pos, category, stereotype, lemma, level = ln.strip().split("\t")
      if level == 'conservative' or not conservative_only:
        hurtlex[pos+"_"+lemma] = category
  return hurtlex

hurtlex_en = __process_hurtlex("hurtlex_EN.tsv")

LST_SYMBOLS = ['⤵', 'ी', 'ு', '্', 'ि', '‡', 'ू', 'ै', '̈', 'ؑ', '●', 'ो', 'ु', 'ಾ', '¦', '▪', 'ು', '°', '༄', '್', '†', '¤', 'ா', 'ॉ', 'ি', '↺', 'ং', 'ื', '‰', '┌', '↓', '┐', '¯', 
               '¸', 'া', '◆', '¬', '˽', '‿', '்', 'ี', 'ं', '่', 'े', '∏', '¨', 'ೆ', '±', '–', '⬇', '\\', 
               'ा', 'ಂ', '○', '।', ')', '्', '⋆', 'ி ','→', 'ে', '़', '□', 'ಿ']
MAP_LETTERS = {'🇴':"O", '🇺':"U", '🆗':"ok", '🇬':"G", '🇨':"C", '🇫':"F", '🇾':"Y", '🇲':"M", '🇹':"T", '🇮':"I", '🅷':"H", '🇰':"K", '🇭':"H", '⏬':"V", '🇯':"J", '🆚':"VS", '🇪':"E", '🇩':"D", '🇦':"A", '🇸':"S", '🇶':"Q", '🆃':"T", '🅰':"A", '🅸':"I", '🇷':"R", '🇻':"V", '🆆':"W", '🇵':"P", '⃣':" ", '🆂':"W", '🇧':"B", '🇽':"X", '🆘':"SOS", '🇳':"n", '🅂':"S", '🇱': "L"}
MAP_OTHER = {'͡':":(", '‹':"<", '》':">", '〝':"\"", '\x81':" ", '\u202a':" ", '\U000e0065':" ", '\u200b':" ", '《':"<", '·':"-", '─':"-", '×':"x", 
             '\U000e0074':" ", '🄸':"I", '∙':"-", '‘':"'", '•':"-", '“':"\"", '»':">", '、':"'", '’':"'", '؛':";", '‼':"!", '̶':"-", '⁉':"?", '℅':"%",
             '~':"~", '—':"-", '„':"\"", '«':"<", '¶':"paragraph", '§':"paragraph", '‚':",", '≠':"different",'™':"Trademark", '…':".", '\u2066':" ", 
             '\u2060':" ", '\u200d':" ", '\U000e0063':" ", '\U000e0067':" ", '\u2063':" ", '\U000e0073':" ", '\U000e006e':" ", '\U000e0062':" ", '\U000e007f':" ", 
             '\xad':" ", '\u2069':" ", '【':"[", '◄':"<", '®':"registered", '・':"-", '´':"'", '©':"copyright", '►':">", '͜':":)", '━':"-", '˜':"~", '！':"!", '⠀':" ", 
             '―':"-", '`':"'", '，':",", 'ِ':"-", '▶':">", '”':"\"", '】':"]", '〞':"\"", '？':"?",   }
def __clean(ln):
  if ln[0] == "\"" and ln[-1] == "\"":
    ln = ln[1:-1]
  for symbol in LST_SYMBOLS:
    ln = ln.replace(symbol, " ")
  for symbol in MAP_LETTERS:
    ln = ln.replace(symbol, " ")
  for symbol in MAP_OTHER:
    ln = ln.replace(symbol, " ")
  ln = ln.replace("’","'").replace("‘","'").replace("´","'").replace("“",'"').replace("”",'"').replace("…",".").replace("ㅤ","-").replace("⋆","*")
  ln = ln.replace("–","-").replace("—","-").replace("⁩","-").replace("❝",'"').replace("❤","*").replace("️","*").replace("●","*").replace("【","[").replace("】","]")
  ln = ln.replace("??‍?","?").replace("??‍","?").replace("!!!","!").replace("!!","!").replace("❞",'"').replace("⚘","*").replace("✓","v").replace("✖","x")
  ln = ln.replace("⬇","!").replace("⁦","")
  ln = ln.replace("\u200b","").replace("\u202a","")
  return ln

def __make_real(text):
  sent = []
  lst_replaces = []
  for word in text.split():
    if word[0] == "$" and word[-1] == "$" and len(word)>5:
      word = word[1:-1]
    if word.startswith("PERSON"):
      lst_replaces.append(word)
      word = "PERSON"
    sent.append(word)
  return " ".join(sent), lst_replaces

LST_EMOJIS = ['🧁', '🤫', '🤮', '🤔', '⏱', '🥇', '🤘', '🤩', '⭐', '🧪', '🧢', '🧠', '🤧', '🥾', '🧕', '🤥', '🦆', '🤒', '🤕', '🤞', '🧐', '⏰', '🥋', '🥺', '🤑', '🤦', '⏳', '🤤', '🥼', '🥔', '🥰', '🤠', '🤨', '🥳', '🤓', '🦝', '🤗', '🦅', '🧚', '🥚', '🤐', '🥂', '🤝', '🤲', '🤢', '🧔', '🤣', '🤟', '🧙', '🤡', '🧀', '🥙', '🧛', '🤛', '🤸', '🥁', '🤯', '🧤', '🤙', '🥊', '🤭', '🤪', '🤷', '🤬']

def load_ds(ds_file):
  total_sents = len(open(ds_file).readlines())
  my_ds = []
  err = []
  dsname = ds_file.replace("only_","").replace(".csv","")
  with open(ds_file, newline='') as input_file_f:
    csv_reader = csv.reader(input_file_f, delimiter=',')
    for index, ln in tqdm(enumerate(csv_reader), total=total_sents):
      lang = "en"
      ds = "hasoc2021"
      splitset = "train"
      source = "twitter"
      if process_set == "test":
        original_id, txt = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = "NONE"
      elif process_set == "train1":
        original_id, txt,task_1, task_2 = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = task_1
      elif process_set == "train2":
        original_id, txt,task_1, task_2 = ln #  ln.strip().split(",") # ,_id,text,task_1,task_2
        cls = task_2
      id = index
      all_words = []
      # this_sent = {"lang":lang, "ds":ds, "id":dsname+"_"+lang+"_"+str(id), "original_id":original_id,"cls":cls, "splitset":splitset, "source":source, "orignal_sent":txt, "parsed":all_words}
      this_sent = {"lang":lang, "ds":ds, "id":original_id, "original_id":original_id,"cls":cls, "splitset":splitset, "source":source, "orignal_sent":txt, "parsed":all_words}
      my_ds.append(this_sent)
      txt = __clean(txt)
      txt, replaces = __make_real(txt)
      # print(replaces)
      # try:
      if True:
        parser = nlp_en
        hurtlex = hurtlex_en
        txt = txt.replace("$HASHTAG$","HASHTAG")
        parsed = parser(txt)
        for sent in parsed.sentences:
          for word in sent.words: # for word, token in zip(sent.words, sent.tokens):
            # print(token)
            text = word.text
            lemma = word.lemma
            pos = word.upos
            feats = (word.feats if word.feats else "_")
            # ner = token.ner
            dep = word.deprel
            head = word.head
            
            if text == "PERSON":
              text = replaces.pop(0)
            if lemma and (pos == "NOUN" or pos == "ADJ"):
              if pos[0].lower() + "_" + lemma in hurtlex:
                hurt = hurtlex[pos[0].lower() + "_" + lemma]
              else:
                hurt = "NA"
            else:
              hurt = "NA"
            emoji_api_lg = lang
            if emoji_api_lg == "de":
              emoji_api_lg = "en"
            if text in LST_EMOJIS:
              text = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
              lemma = "EMOJI"
            else:
              aux = tweet_preprocessor.parse(text)
              if aux.emojis:
                for val in aux.emojis:
                  emoji = val.match
                  newemoji = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
                  text = emoji
                  lemma = "EMOJI"
              elif aux.smileys:
                for val in aux.smileys:
                  emoji = val.match
                  newemoji = emoji_api.demojize(emoji, language=emoji_api_lg).replace(":","")
                  text = emoji
                  lemma = "EMOJI"
            if text in MAP_LETTERS:
              text = MAP_LETTERS[text]
              lemma = "LETTER"
            this_word = {"text":text, "lemma":lemma, "pos":pos, "feats":feats, "dep":dep,"head":head, "hurtlex": hurt}
            all_words.append(this_word)
        # print(replaces, all_words)
        assert len(replaces) == 0
      # except Exception as inst:
      #   print(inst.args)
      #   print(inst)  
      #   err.append(ln)
  return my_ds, err

for file_path in ds_files:
  print(file_path)
  # print(file_path+".pkl")
  ds, err = load_ds(file_path)
  print("...errs", len(err))
  pickle.dump([ds, err], open(file_path+".pkl", 'wb'))



